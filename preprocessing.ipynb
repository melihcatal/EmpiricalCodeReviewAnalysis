{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7d2be8f",
   "metadata": {},
   "source": [
    "ESE P6: An Empirical Study of GitHub code review tool \n",
    "by Eleonora Pura  and Melih Catal\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70611a51",
   "metadata": {},
   "source": [
    "The goal of the preprocessing steps is to building two datasets of triplets (<ms,rnl>->mr) and pairs (ms->mr) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "63adb053",
   "metadata": {},
   "source": [
    "## Data Mining\n",
    "This step is skipped because the data is already ready and given by the tutor."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "541b171d",
   "metadata": {},
   "source": [
    "## Methods Extraction and Linking Reviewer Comment\n",
    "Parsing Java files using the Lizard Python library to extract the methods. We are only interested in the java files.\n",
    "Link each comment to the specific method (if any) it refers to. If a comment cannot be linked to any method, it is discarded. After having linked comments to methods for each review round, we are in the situation in which we have, for each review round, a set of triplets <ms,mr and {Rnl}> where ms and mr represent the same method before and after the review round, and Rnl is a set of comments ms received in this round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00793b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lizard\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')  # download the stop words corpus\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a378abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_function_body(source_code, start_line, end_line):\n",
    "    lines = source_code.split('\\n')[start_line-1: end_line+1]\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "\n",
    "def comment_in_method_body(comment_start_line, comment_end_line, function_start_line, function_end_line):\n",
    "    comment_range = set(range(comment_start_line, comment_end_line+1))\n",
    "    function_range = set(range(function_start_line, function_end_line+1))\n",
    "    \n",
    "    if comment_range.issubset(function_range):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "\n",
    "def is_given_by_reviewer_not_author(owner_id, user_id):\n",
    "    if owner_id != user_id:\n",
    "        return True # since author is not commenting on his own code\n",
    "    else:\n",
    "        return False # since author is commenting on his own code\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a451d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all the function from a row of the df\n",
    "def extract_row_functions(row, file_content):\n",
    "    functions = []\n",
    "    # if the comment start line is na that means the comment is a single line comment so we use the end line number as the start line number\n",
    "    comment_start_line = row['original_start_line'] if not pd.isna(row['original_start_line']) else row['original_line']\n",
    "    comment_end_line = row['original_line']\n",
    "    # Check that the comment is given by a reviewer\n",
    "    if  is_given_by_reviewer_not_author(row['owner_id'], row['user_id']):\n",
    "        # Check that both start and end line are not NaN floats\n",
    "        if (not pd.isna(comment_start_line)) and (not pd.isna(comment_end_line)):\n",
    "            # Convert all start and end line to int \n",
    "            comment_start_line = int(comment_start_line)\n",
    "            comment_end_line = int(comment_end_line)\n",
    "\n",
    "            # Extract all the functions from the file\n",
    "            l = lizard.analyze_file.analyze_source_code(row['filename'], row[file_content])\n",
    "            l_functions = l.function_list\n",
    "\n",
    "            # \n",
    "            for function in l_functions:\n",
    "                function_start_line = int(function.start_line)\n",
    "                function_end_line = int(function.end_line)\n",
    "                \n",
    "                # Check that the comment is part of the body of the function\n",
    "                if comment_in_method_body(comment_start_line, comment_end_line, function_start_line, function_end_line):\n",
    "                    body = find_function_body(row[file_content], function_start_line, function_end_line)\n",
    "                    \n",
    "                    functions.append(\n",
    "                        {\"name\": function.name,\n",
    "                         \"long_name\": function.long_name,\n",
    "                         \"start_line\": int(function.start_line),\n",
    "                         \"end_line\": int(function.end_line),\n",
    "                         \"body\": body})\n",
    "    return functions\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12190961",
   "metadata": {},
   "outputs": [],
   "source": [
    "def methods_extraction_linking_reviewer_comments(df):\n",
    "    # Make a copy of the original df\n",
    "    df_copy = df.copy()\n",
    "    functions_while = []\n",
    "    functions_after = []\n",
    "    for idx, row in df_copy.iterrows():\n",
    "        row_functions_while = extract_row_functions(row, 'file_content_while')\n",
    "        row_functions_after = extract_row_functions(row, 'file_content_after')\n",
    "        \n",
    "        functions_while.append(row_functions_while)\n",
    "        functions_after.append(row_functions_after)\n",
    "    # Add 2 new columns to df: functions_while and functions_after\n",
    "    df_copy['functions_while'] = functions_while\n",
    "    df_copy['functions_after'] = functions_after\n",
    "\n",
    "    # Set all empty list values as np.nan in order to use pandas dropna function\n",
    "    df_copy.functions_while = df_copy.functions_while.apply(lambda y: np.nan if len(y)==0 else y)\n",
    "    df_copy.functions_after = df_copy.functions_after.apply(lambda y: np.nan if len(y)==0 else y)\n",
    "\n",
    "    # Delete all entries in which there is no functions while or after\n",
    "    df_copy = df_copy.dropna(subset=['functions_while', 'functions_after']).reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # If the functions while and after remains the same that means that no error was fixed: remove from df\n",
    "    df_copy = df_copy[df_copy['functions_while'] != df_copy['functions_after']].reset_index(drop=True)\n",
    "\n",
    "    # Turn the dataframe in a list of triplets: (function_while, comment, function_after)\n",
    "    triplets = []\n",
    "\n",
    "    for idx, row in df_copy.iterrows():\n",
    "        if len(row['functions_while']) == len(row['functions_after']):\n",
    "            for function_while, function_after in zip(row['functions_while'], row['functions_after']):\n",
    "                triplets.append((function_while['body'], row['message'], function_after['body']))\n",
    "    \n",
    "    return triplets\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c21732a5",
   "metadata": {},
   "source": [
    "## Abstraction\n",
    "Use `src2abs` tool to abstract the methods. Triplets for which a parsing error occur during the abstraction process on the ms or on the mr methods are removed from the dataset. ms and mr, after the abstraction must be different. Idioms are not abstracted. During the abstraction code comments are removed. Using the pair abstraction mode, the same literals/identifiers in the two methods weill be abstracted using the same IDs. As output of the abstract process, the tool provides an abstraction map M linking the abstracted token to the raw token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d15abb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_java_file(content, filename):\n",
    "    path = f\"./abstraction/java_files/{filename}\"\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19370a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_abstraction_file():\n",
    "    code_granularity = \"method\"\n",
    "    mode = \"pair\"\n",
    "    idioms_path = \"./src2abs/idioms.csv\"\n",
    "    src2abs_path = \"./src2abs/src2abs-0.1-jar-with-dependencies.jar\"\n",
    "    input_code_A_path = \"./abstraction/java_files/ms.java\"\n",
    "    input_code_B_path = \"./abstraction/java_files/mr.java\"\n",
    "    output_abstract_A_path = \"./abstraction/abstraction_files/ms_abs.java\"\n",
    "    output_abstract_B_path = \"./abstraction/abstraction_files/mr_abs.java\"\n",
    "    try :\n",
    "        # java -jar src2abs-0.1-jar-with-dependencies.jar pair <code_granularity> <input_code_A_path> <input_code_B_path> <output_abstract_A_path> <output_abstract_B_path> <idioms_path>\n",
    "        command = ['java', '-jar', src2abs_path, mode, code_granularity, input_code_A_path, input_code_B_path, output_abstract_A_path, output_abstract_B_path, idioms_path]\n",
    "        # execute the command and capture the output\n",
    "        process = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        stdout, stderr = process.communicate()\n",
    "        if stderr:\n",
    "            #print(\"Error creating abstraction data\")\n",
    "            #print(stderr)\n",
    "            pass\n",
    "        #else :\n",
    "            #print(\"Abstraction data created\")\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        #print(\"Error creating abstraction data\")\n",
    "        #print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f870066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_abstraction_file(filename):\n",
    "    path = f\"./abstraction/abstraction_files/{filename}\"\n",
    "    with open(path, 'r') as f:\n",
    "        return f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982b754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read mapping file and create a dictionary with the mapping. key: concrete, value: abstract\n",
    "def read_abstraction_mapping_file(filename):\n",
    "    path = f\"./abstraction/abstraction_files/{filename}\"\n",
    "    with open(path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    is_abstract = False\n",
    "    keys = []\n",
    "    values = []\n",
    "    for line in lines:\n",
    "        words = line.strip().split(',')\n",
    "        if len(words) > 1 :\n",
    "            for word in words:\n",
    "                if word == '':\n",
    "                    continue\n",
    "                if is_abstract:\n",
    "                    values.append(word)\n",
    "                else:\n",
    "                    keys.append(word)\n",
    "            is_abstract = not is_abstract\n",
    "     # convert the lists to a dictionary\n",
    "    maps = dict(zip(keys, values))\n",
    "\n",
    "\n",
    "    return maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26687581",
   "metadata": {},
   "outputs": [],
   "source": [
    "def abstraction_filter_methods(ms,mr):\n",
    "    # ms and mr should be different \n",
    "    if ms == mr:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "        \n",
    "def clean_rnl(rnl):\n",
    "    # remove links\n",
    "    rnl = re.sub(r'http\\S+', '', rnl)\n",
    "    # superfluous punctuation\n",
    "    rnl = re.sub(r'[^\\w\\s]', '', rnl)\n",
    "    return rnl\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b0f637",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "template_triplets_data = {\n",
    "    \"ms\": \"\",\n",
    "    \"comment\": \"\",\n",
    "    \"mr\": \"\"\n",
    "    \"ms_abs\": \"\",\n",
    "    \"mr_abs\": \"\",\n",
    "    \"map\" : dict()\n",
    "}\n",
    "ms = methods while\n",
    "mr = methods after change\n",
    "rnl = review comment\n",
    "'''\n",
    "def create_abstraction_data(ms,mr,rnl):\n",
    "    template_triplets_data = {}\n",
    "    create_java_file(ms, \"ms.java\")\n",
    "    create_java_file(mr, \"mr.java\")\n",
    "    create_abstraction_file()\n",
    "    ms_abs = read_abstraction_file(\"ms_abs.java\")\n",
    "    mr_abs = read_abstraction_file(\"mr_abs.java\")\n",
    "    mapping = read_abstraction_mapping_file(\"ms_abs.java.map\")\n",
    "    rnl = clean_rnl(rnl)\n",
    "    \n",
    "    template_triplets_data[\"ms\"] = ms\n",
    "    template_triplets_data[\"comment\"] = rnl \n",
    "    template_triplets_data[\"mr\"] = mr\n",
    "    template_triplets_data[\"ms_abs\"] = ms_abs\n",
    "    template_triplets_data[\"mr_abs\"] = mr_abs\n",
    "    template_triplets_data[\"map\"] = mapping\n",
    "    if abstraction_filter_methods(ms_abs, mr_abs):\n",
    "        #triplets_with_abstraction.append(template_triplets_data)\n",
    "        return template_triplets_data\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c4ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def methods_abstraction(triplets):\n",
    "    triplets_with_abstraction = []\n",
    "    for triplet in triplets:\n",
    "        abstracted_method = create_abstraction_data(triplet[0], triplet[2], triplet[1])\n",
    "        if abstracted_method:\n",
    "            triplets_with_abstraction.append(abstracted_method)\n",
    "    return triplets_with_abstraction\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "16ddd69e",
   "metadata": {},
   "source": [
    "## Abstracting Reviewer Comments \n",
    "Abstract all code components mentioned in any comment using the abstraction map. Any camel casse identifier that is not matched in the abstraction map but that it is present in the comment, is replaced by the special token _CODE_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d77da60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_camel_case(s):\n",
    "    # example: \"camelCase\", \"camelcase\", \"camelCaseCase\", \"camelCaseC\"\n",
    "    # not example: \"camel\", \"camelCase_\"\n",
    "    pattern = r'^(?:[A-Z][a-z]*|[a-z]+)((?:[A-Z][a-z]*))+$'\n",
    "    return bool(re.match(pattern, s))\n",
    "\n",
    "def remove_stop_words(comment):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = comment.split()\n",
    "    filtered_comment = [w for w in words if not w in stop_words]\n",
    "    return \" \".join(filtered_comment)\n",
    "\n",
    "\n",
    "def abstraction_filter_rnl(rnl):\n",
    "    if len(rnl.split()) > 100:\n",
    "        return False\n",
    "    else :\n",
    "        return True\n",
    "\n",
    "def abstract_reviewer_comments(triplets_with_abstraction):\n",
    "    camel_case_abs = \"_CODE_\"\n",
    "    for triplet in triplets_with_abstraction:\n",
    "        comment = triplet[\"comment\"]\n",
    "        mapping = triplet[\"map\"]\n",
    "        comment_words = comment.split()\n",
    "        for word in comment_words:\n",
    "            # remove any special characters from the word\n",
    "            word = re.sub(r'[^\\w\\s]', '', word)\n",
    "            if word in mapping:               \n",
    "                comment = comment.replace(word, mapping[word])\n",
    "            elif is_camel_case(word):\n",
    "                comment = comment.replace(word, camel_case_abs)\n",
    "        comment = remove_stop_words(comment)\n",
    "\n",
    "        if abstraction_filter_rnl(comment):\n",
    "            triplet[\"comment_abs\"] = comment\n",
    "        else :\n",
    "            # delete the triplet\n",
    "            triplets_with_abstraction.remove(triplet)\n",
    "\n",
    "    return triplets_with_abstraction\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cbde39c",
   "metadata": {},
   "source": [
    "## Filtering out Noisy Comments\n",
    "Using heuristics to filter out noisy comments.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ba47b36d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14de8adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_relevant(comment):\n",
    "    comment_size = len(comment.split())\n",
    "\n",
    "    is_relevant = True\n",
    "    # Useless comments, no content after removing stopwords\n",
    "    if len(comment) == 0:\n",
    "        is_relevant = False\n",
    "\n",
    "    # Useless comments, one word, no action required or unclear action\n",
    "    elif comment_size == 1:\n",
    "        if comment.__contains__('nice') or comment.__contains__('pleas') \\\n",
    "            or comment.__contains__('ditto') or comment.__contains__('thank') \\\n",
    "            or comment.__contains__('ditto2') or comment.__contains__('fine') \\\n",
    "            or comment.__contains__('agew') or comment.__contains__('hahaha') \\\n",
    "            or comment.__contains__('yeh') or comment.__contains__('lol'):\n",
    "            is_relevant = False\n",
    "\n",
    "    elif comment_size == 2:\n",
    "        if comment.__contains__('ack'):\n",
    "            is_relevant =False\n",
    "\n",
    "    # Request to change formatting, no impact on code\n",
    "    elif comment.__contains__('indent') and comment_size < 5:\n",
    "        is_relevant = False\n",
    "\n",
    "    # Likely a thank you message\n",
    "    elif (comment.__contains__('works for me') or comment.__contains__('sounds good') \\\n",
    "        or comment.__contains__('makes sense') or comment.__contains__('smile') \\\n",
    "        or comment.__contains__('approv')) and comment_size < 5:\n",
    "        is_relevant = False\n",
    "\n",
    "    # Request to add test code, no impact on the reviewed code\n",
    "    elif (comment.__contains__('test') and comment_size < 5) \\\n",
    "        or (comment.__contains__('add') and comment.__contains__('test')):\n",
    "        is_relevant = False\n",
    "\n",
    "    # Request for clarification\n",
    "    elif ((comment.__contains__('please explain') or comment.__contains__('explan') \\\n",
    "            or comment.__contains__('wat') or comment.__contains__('what')) and comment_size < 5) \\\n",
    "        or ((comment.__contains__('understand') or comment.__contains__('meant')) \\\n",
    "            and comment.__contains__('not sure')):\n",
    "        is_relevant = False\n",
    "\n",
    "    # Refers to previous comment or external resource with unclear action point\n",
    "    elif (comment.__contains__('same as') or comment.__contains__('same remark') \\\n",
    "            or comment.__contains__('said above') or comment.__contains__('do the same')) \\\n",
    "        and comment_size < 5:\n",
    "        is_relevant = False\n",
    "\n",
    "    # Refers to web pages\n",
    "    elif (comment.__contains__('like') or comment.__contains__('see')) \\\n",
    "        and comment.__contains__('http'):\n",
    "        is_relevant = False\n",
    "\n",
    "    # Request to add comment\n",
    "    elif comment.__contains__('document') or comment.__contains__('javadoc') \\\n",
    "            or comment.__contains__('comment'):\n",
    "        is_relevant =  False\n",
    "\n",
    "    # Feedback about reorganizing the PR\n",
    "    elif comment.__contains__('pr') and comment_size < 5:\n",
    "        is_relevant = False\n",
    "\n",
    "    # Comment contains a +1 to support previous comment.\n",
    "    # It may be accompanied by another word, like agree or a smile.\n",
    "    # This is the reason for < 3\n",
    "    elif comment.__contains__('+1') and comment_size < 3:\n",
    "        is_relevant = False\n",
    "\n",
    "    # The code is ok for now\n",
    "    elif comment.__contains__('for now') and comment_size < 5:\n",
    "        is_relevant = False\n",
    "\n",
    "    # Answers\n",
    "    elif (comment.__contains__('fixed') or comment.__contains__('thank') \\\n",
    "            or comment.__contains__('youre right')) and comment_size < 3:\n",
    "        is_relevant = False\n",
    "    \n",
    "    return is_relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f491687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell is for testing the is_relevant function and calculating the percentage of irrelevant comments among all comments\n",
    "# TODO: remove this cell when done testing\n",
    "\n",
    "# run is_relevant on all comments and get number of irrelevant comments \n",
    "def get_irrelevant_comments(triplets_with_abstraction):\n",
    "    irrelevant_comments = 0\n",
    "    for triplet in triplets_with_abstraction:\n",
    "        # check if the key exists\n",
    "        if \"comment_abs\" not in triplet:\n",
    "            continue\n",
    "        comment_abs = triplet[\"comment_abs\"]\n",
    "        if not is_relevant(comment_abs):\n",
    "            irrelevant_comments += 1\n",
    "    return irrelevant_comments\n",
    "\n",
    "\n",
    "#irrelevant_comments = get_irrelevant_comments(triplets_with_abstraction)\n",
    "#print(irrelevant_comments)\n",
    "#print(len(triplets_with_abstraction) - irrelevant_comments)\n",
    "# percentage of irrelevant comments, the paper says in their dataset it is around 11%\n",
    "#print(irrelevant_comments / len(triplets_with_abstraction) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c12740d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove irrelevant comments\n",
    "def remove_irrelevant_comments(triplets_with_abstraction):\n",
    "    for triplet in triplets_with_abstraction:\n",
    "        # check if the key exists\n",
    "        if \"comment_abs\" not in triplet:\n",
    "            continue\n",
    "        comment_abs = triplet[\"comment_abs\"]\n",
    "        if not is_relevant(comment_abs):\n",
    "            triplets_with_abstraction.remove(triplet)\n",
    "    return triplets_with_abstraction\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "29ef6ee3",
   "metadata": {},
   "source": [
    "## Running the pipeline over all the GitHub Projects\n",
    "\n",
    "We ran the pipeline over all the GitHub projects. As a first approach, we used multithreading to make the process faster. However, we were skeptical about the correctness of the results. For this reason, we decided to run the pipeline sequentially. And we compared the results. The dataset obtained using the multithreading approach has 175829 triplets and the dataset obtained using sequential approach has 171669 triplets. The difference is not so big, but we decided to use the dataset obtained using the sequential approach since we were able to find some mistakes in the multithreading approach during the comparison. Both approaches and the dataset are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390518e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create <ms,mr,rnl> triple from the triplets_with_abstraction\n",
    "def create_ms_mr_rnl_triple(triplets_with_abstraction):\n",
    "    ms_mr_rnl_triplets = []\n",
    "    for triplet in triplets_with_abstraction:\n",
    "        try: \n",
    "            ms_mr_rnl_triplets.append({\n",
    "                \"ms\": triplet[\"ms\"],\n",
    "                \"mr\": triplet[\"mr\"],\n",
    "                \"rnl\": triplet[\"rnl\"]\n",
    "            })\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return ms_mr_rnl_triplets\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd823f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv(path):\n",
    "    df = pd.read_csv(path)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75837ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This cell is used to run the pipeline over all the files in the github projects folder using multithreading. \n",
    "\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def process_file(filename):\n",
    "    triplets = []\n",
    "    try:\n",
    "        path = os.path.join(folder, filename)\n",
    "        df = read_csv(path)\n",
    "        triplets = methods_extraction_linking_reviewer_comments(df)\n",
    "        triplets_with_abstracted_methods = methods_abstraction(triplets)\n",
    "        triplets_with_abstraction = abstract_reviewer_comments(triplets_with_abstracted_methods)\n",
    "        # remove irrelevant comments\n",
    "        triplets_with_abstraction = remove_irrelevant_comments(triplets_with_abstraction)\n",
    "        # create <ms,mr,rnl> triple from the triplets_with_abstraction\n",
    "        current_ms_mr_rnl_triplet = create_ms_mr_rnl_triple(triplets_with_abstraction)\n",
    "        triplets.extend(current_ms_mr_rnl_triplet)\n",
    "    except Exception as e:\n",
    "        print(\"e\")\n",
    "        #return triplets, 1, filename\n",
    "    return triplets, 0, filename\n",
    "\n",
    "def parallel_process_files(folder, num_threads=4):\n",
    "    filepaths = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "    ms_mr_rnl_triplets = []\n",
    "    num_errors = 0\n",
    "    total_files = len(filepaths)\n",
    "    completed_files = 0\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=num_threads) as executor:\n",
    "        futures = {executor.submit(process_file, f): f for f in filepaths}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            triplets, errors, filename = future.result()\n",
    "            ms_mr_rnl_triplets.extend(triplets)\n",
    "            num_errors += errors\n",
    "            completed_files += 1\n",
    "            print(f\"Finished processing file: {filename} ({completed_files}/{total_files})\")\n",
    "            \n",
    "    return ms_mr_rnl_triplets, num_errors\n",
    "\n",
    "folder = \"data3\"\n",
    "num_threads = 3\n",
    "\n",
    "final_triple, num_errors = parallel_process_files(folder, num_threads)\n",
    "print(\"Number of errors: \" + str(num_errors))\n",
    "''' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75484046",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this cell is used to run the pipeline over all the files in the github projects folder. It is not using multithreading\n",
    "def main():\n",
    "    #path = './data/gh_apache_!_accumulo.csv'\n",
    "    folder = \"data\"\n",
    "    # number of files in the folder\n",
    "    num_files = len(os.listdir(folder))\n",
    "    # current file number\n",
    "    file_num = 1\n",
    "    ms_mr_rnl_triplets = []\n",
    "    num_erros = 0\n",
    "    # for each file in the folder run the process\n",
    "    for filename in os.listdir(folder):\n",
    "        # skip the first 550 files and end the process at 600 files\n",
    "        try:\n",
    "            print (\"Processing file \" + str(file_num) + \" out of \" + str(num_files))\n",
    "            path = os.path.join(folder, filename)\n",
    "            df = read_csv(path)\n",
    "            triplets = methods_extraction_linking_reviewer_comments(df)\n",
    "            triplets_with_abstracted_methods =  methods_abstraction(triplets)\n",
    "            triplets_with_abstraction = abstract_reviewer_comments(triplets_with_abstracted_methods)\n",
    "            # remove irrelevant comments\n",
    "            triplets_with_abstraction = remove_irrelevant_comments(triplets_with_abstraction)\n",
    "\n",
    "            # create <ms,mr,rnl> triple from the triplets_with_abstraction\n",
    "            current_ms_mr_rnl_triplet = create_ms_mr_rnl_triple(triplets_with_abstraction)\n",
    "            ms_mr_rnl_triplets.extend(current_ms_mr_rnl_triplet)\n",
    "            print(\"Finished processing file \" + str(file_num) + \" out of \" + str(num_files))\n",
    "            print(\"*******************\")\n",
    "            file_num += 1\n",
    "        except Exception as e:\n",
    "            num_erros += 1\n",
    "            file_num += 1\n",
    "            continue\n",
    "    print(\"Number of errors: \" + str(num_erros))\n",
    "    return ms_mr_rnl_triplets\n",
    "\n",
    "final_triple = main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968566b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(final_triple))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e590889c",
   "metadata": {},
   "source": [
    "## Export the final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc000627",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# export the data to a json file\n",
    "import json\n",
    "with open('ms_mr_rnl_dataset.json', 'w') as fp:\n",
    "    json.dump(final_triple, fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424bf308",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(final_triple, columns=['ms', 'mr', 'rnl'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b5bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('ms_mr_rnl_dataset.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70d02c8e",
   "metadata": {},
   "source": [
    "## Additional Filters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d9172e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "# 1. remove mr and ms the same\n",
    "def filter_remove_same_ms_mr(df):\n",
    "    prior_len = df.shape[0]\n",
    "    df = df[df['ms_abs'] != df['mr_abs']]\n",
    "    print(\"Removed {} rows where ms and mr are the same\".format(prior_len - df.shape[0]))\n",
    "    print(\"prior was {} and now is {}\".format(prior_len, df.shape[0]))\n",
    "    return df\n",
    "\n",
    "# 2. remove group by ms,mr and rnl more than 1\n",
    "def filter_remove_multiple_rnl(df):\n",
    "    prior_len = df.shape[0]\n",
    "    df = df.groupby(['ms_abs', 'mr_abs']).filter(lambda x: len(x) == 1)\n",
    "    print(\"Removed {} rows where rnl set has more than one comment\".format(prior_len - df.shape[0]))\n",
    "    print(\"prior was {} and now is {}\".format(prior_len, df.shape[0]))\n",
    "    return df\n",
    "\n",
    "# 3. remove ms or mr tokens longer than 100\n",
    "def filter_tokens_length(df):\n",
    "    prior_len = df.shape[0]\n",
    "    df['ms_tokens'] = df['ms_abs'].apply(tokenize)\n",
    "    df['mr_tokens'] = df['mr_abs'].apply(tokenize)\n",
    "    df['ms_tokens_length'] = df['ms_tokens'].apply(len)\n",
    "    df['mr_tokens_length'] = df['mr_tokens'].apply(len)\n",
    "\n",
    "    # remove the triplets that have ms or mr tokens length more than 100\n",
    "    df = df[df['ms_tokens_length'] <= 100]\n",
    "    df = df[df['mr_tokens_length'] <= 100]\n",
    "\n",
    "    print(\"Removed {} rows where ms or mr tokens length more than 100\".format(prior_len - df.shape[0]))\n",
    "    print(\"prior was {} and now is {}\".format(prior_len, df.shape[0]))\n",
    "    return df\n",
    "\n",
    "# 4. mr contains literals that are not present in the ms\n",
    "def check_abstracted_literals(text):\n",
    "    pattern = r'\\b[A-Z_0-9]+\\b'\n",
    "    return re.findall(pattern, text)\n",
    "\n",
    "def find_unmatched_literals(row):\n",
    "    return set(row['mr_abstracted_literals']) - set(row['ms_abstracted_literals'])\n",
    "\n",
    "def filter_literals_not_present_in_ms(df):\n",
    "    prior_len = df.shape[0]\n",
    "    df['ms_abstracted_literals'] = df['ms_abs'].apply(check_abstracted_literals)\n",
    "    df['mr_abstracted_literals'] = df['mr_abs'].apply(check_abstracted_literals)\n",
    "    df['unmatched_literals'] = df.apply(find_unmatched_literals, axis=1)\n",
    "    df = df[df['unmatched_literals'].apply(len) == 0]\n",
    "    print(\"Removed {} rows where mr contains literals that are not present in the ms\".format(prior_len - df.shape[0]))\n",
    "    print(\"prior was {} and now is {}\".format(prior_len, df.shape[0]))\n",
    "    return df\n",
    "\n",
    "# change column names to ms, mr, comment\n",
    "def create_triplets_with_map_df(df):\n",
    "    ms_mr_rnl_map_df = df[['ms_abs', 'mr_abs', 'comment_abs','map']] \n",
    "    ms_mr_rnl_map_df.columns = ['ms', 'mr', 'rnl', 'map']\n",
    "    ms_mr_rnl_map_df.reset_index(drop=True, inplace=True)\n",
    "    ms_mr_rnl_map_df.head()\n",
    "    return ms_mr_rnl_map_df\n",
    "\n",
    "def additional_filters(df):\n",
    "    df = filter_remove_same_ms_mr(df)\n",
    "    df = filter_remove_multiple_rnl(df)\n",
    "    df = filter_tokens_length(df)\n",
    "    df = filter_literals_not_present_in_ms(df)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf67077a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here instead of running the main function, we load the data from the csv file that we created in the previous step and apply the filters\n",
    "\n",
    "df = read_csv(\"./raw_data.csv\")\n",
    "triplets = methods_extraction_linking_reviewer_comments(df)\n",
    "\n",
    "triplets_with_abstracted_methods =  methods_abstraction(triplets)\n",
    "\n",
    "triplets_with_abstraction = abstract_reviewer_comments(triplets_with_abstracted_methods)\n",
    "\n",
    "triplets_with_abstraction = remove_irrelevant_comments(triplets_with_abstraction)\n",
    "\n",
    "triplets_with_abstraction_df = pd.DataFrame(triplets_with_abstraction)\n",
    "\n",
    "triplets_with_abstraction_df = additional_filters(triplets_with_abstraction_df)\n",
    "\n",
    "triplets_with_abstraction_df = create_triplets_with_map_df(triplets_with_abstraction_df)\n",
    "\n",
    "final_triplets = create_ms_mr_rnl_triple(triplets_with_abstraction_df)\n",
    "\n",
    "ms_mr_rnl_dataset = pd.DataFrame(final_triplets, columns=['ms', 'mr', 'rnl'])\n",
    "ms_mr_dataset = pd.DataFrame(final_triplets, columns=['ms', 'mr'])\n",
    "\n",
    "ms_mr_rnl_dataset.to_csv(\"ms_mr_rnl_dataset.csv\", index=False)\n",
    "ms_mr_dataset.to_csv(\"ms_mr_dataset.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca39f90c",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "949d0b6b",
   "metadata": {},
   "source": [
    "## Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6ca4416e",
   "metadata": {},
   "source": [
    "### Prepare the datasets for 1-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a9093",
   "metadata": {},
   "outputs": [],
   "source": [
    "oneenc_dataset = pd.read_csv('./data/ms_mr_dataset.csv')\n",
    "\n",
    "# Randomly shuffle df to have random train, test and validation sets\n",
    "oneenc_dataset = shuffle(oneenc_dataset).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26e75d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = oneenc_dataset.mr.to_frame()\n",
    "X = oneenc_dataset.drop('mr', axis=1)\n",
    "\n",
    "# Split the dataset: 80% for the training and 20% for the rest (test and validation)\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.2, shuffle=False, random_state=0)\n",
    "\n",
    "#Further divide the rest (20%) into test (10%) and validation (10%) datasets \n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_rest, y_rest, test_size=0.5, shuffle=False, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4297817",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4ab43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Take dataframe and path of a txt file and write all df row of the first column in a new line of the txt file\"\"\"\n",
    "def create_txt_file(df, path):\n",
    "    file = open(path, \"w\", encoding=\"utf-8\")\n",
    "\n",
    "    # If it is the last element of the dataframe don't add the new line\n",
    "    for idx, row in df.iterrows():\n",
    "        if idx == df.index[-1]:\n",
    "            file.write(f\"{row[0]}\")\n",
    "\n",
    "        else:\n",
    "            file.write(f\"{row[0]}\\n\")\n",
    "\n",
    "    file.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8eacb",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_train = './data/1-encoder/train/src-train.txt'\n",
    "tgt_train = './data/1-encoder/train/tgt-train.txt'\n",
    "\n",
    "src_test = './data/1-encoder/test/src-test.txt'\n",
    "tgt_test = './data/1-encoder/test/tgt-test.txt'\n",
    "\n",
    "\n",
    "src_val = './data/1-encoder/eval/src-val.txt'\n",
    "tgt_val = './data/1-encoder/eval/tgt-val.txt'\n",
    "\n",
    "\n",
    "create_txt_file(X_train, src_train)\n",
    "create_txt_file(y_train, tgt_train)\n",
    "\n",
    "create_txt_file(X_test, src_test)\n",
    "create_txt_file(y_test, tgt_test)\n",
    "\n",
    "create_txt_file(X_valid, src_val)\n",
    "create_txt_file(y_valid, tgt_val)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "71c022e2",
   "metadata": {},
   "source": [
    "### Prepare the datasets for 2-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "twoenc_dataset = pd.read_csv('./data/ms_mr_rnl_dataset.csv')\n",
    "# Randomly shuffle df to have random train, test and validation sets\n",
    "twoenc_dataset = shuffle(twoenc_dataset).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db63485d",
   "metadata": {},
   "outputs": [],
   "source": [
    "twoenc_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef265d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = twoenc_dataset.mr.to_frame()\n",
    "X = twoenc_dataset.drop('mr', axis=1)\n",
    "\n",
    "\n",
    "# Split the dataset: 80% for the training and 20% for the rest (test and validation)\n",
    "X_train, X_rest, y_train, y_rest = train_test_split(X, y, test_size=0.2, shuffle=0, random_state=0)\n",
    "\n",
    "#Further divide the rest (20%) into test (10%) and validation (10%) datasets \n",
    "X_test, X_valid, y_test, y_valid = train_test_split(X_rest, y_rest, test_size=0.5, shuffle=0, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2484ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "src1_train = './data/2-encoder/train/src1-train.txt'\n",
    "src2_train = './data/2-encoder/train/src2-train.txt'\n",
    "tgt_train = './data/2-encoder/train/tgt-train.txt'\n",
    "\n",
    "\n",
    "src1_test = './data/2-encoder/test/src1-test.txt'\n",
    "src2_test = './data/2-encoder/test/src2-test.txt'\n",
    "tgt_test = './data/2-encoder/test/tgt-test.txt'\n",
    "\n",
    "src1_val = './data/2-encoder/eval/src1-val.txt'\n",
    "src2_val = './data/2-encoder/eval/src2-val.txt'\n",
    "tgt_val = './data/2-encoder/eval/tgt-val.txt'\n",
    "\n",
    "\n",
    "# For the input X we create a copy of the dataset depending on the needed column (ms or rnl)\n",
    "create_txt_file(X_train[['ms']].copy(), src1_train)\n",
    "create_txt_file(X_train[['rnl']].copy(), src2_train)\n",
    "create_txt_file(y_train, tgt_train)\n",
    "\n",
    "create_txt_file(X_test[['ms']].copy(), src1_test)\n",
    "create_txt_file(X_test[['rnl']].copy(), src2_test)\n",
    "create_txt_file(y_test, tgt_test)\n",
    "\n",
    "create_txt_file(X_valid[['ms']].copy(), src1_val)\n",
    "create_txt_file(X_valid[['rnl']].copy(), src2_val)\n",
    "create_txt_file(y_valid, tgt_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
